{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1a0ebf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from denoising_diffusion_pytorch import GaussianDiffusion\n",
    "from denoising_diffusion_pytorch.denoising_diffusion_pytorch import SinusoidalPosEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aaacc943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneDModel(torch.nn.Module):\n",
    "    def __init__(self, dim = 16, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.channels = self.out_dim = 1\n",
    "        self.self_condition = False\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        sinu_pos_emb = SinusoidalPosEmb(dim)\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, dim)\n",
    "        )\n",
    "        \n",
    "        self.main = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(1, dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(dim, dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(dim, dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(dim, dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(dim, 1)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x_t, t, self_cond=None):\n",
    "        \"\"\" x: (batsize, 1, 1, 1)\"\"\"\n",
    "        x_t = x_t.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        temb = self.time_mlp(t)\n",
    "        \n",
    "        h = self.main[0](x_t)\n",
    "        h = h + temb\n",
    "        for layer in self.main[1:]:\n",
    "            h = layer(h)\n",
    "            \n",
    "        h = h[:, :, None, None]\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3a3dd122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.8151]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4265]]],\n",
      "\n",
      "\n",
      "        [[[-2.3414]]],\n",
      "\n",
      "\n",
      "        [[[-0.1745]]],\n",
      "\n",
      "\n",
      "        [[[-0.0328]]]]) tensor([0.4725, 0.7170, 0.4406, 0.3731, 0.4615])\n",
      "tensor([[[[-0.0830]]],\n",
      "\n",
      "\n",
      "        [[[-0.0906]]],\n",
      "\n",
      "\n",
      "        [[[-0.0724]]],\n",
      "\n",
      "\n",
      "        [[[-0.0879]]],\n",
      "\n",
      "\n",
      "        [[[-0.0888]]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = OneDModel()\n",
    "x = torch.randn((5,1, 1, 1))\n",
    "t = torch.rand((5,))\n",
    "print(x, t)\n",
    "\n",
    "y = m(x, t)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1d80e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = GaussianDiffusion(model=m, image_size=1, timesteps=100, loss_type=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dbdee34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.peaks = [-3, -2, 2, 3]\n",
    "        self.var = 0.05\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        # randomly choose one of the peaks\n",
    "        ret = random.choice(self.peaks)\n",
    "        ret = ret + random.gauss(0, self.var)\n",
    "        return torch.tensor([ret])[:, None, None]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 10000\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3348d921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "ds = OneDDataset()\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=128)\n",
    "\n",
    "print(len(ds))\n",
    "samples = [x[0, 0, 0] for x in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "abcdd889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.45800964e-02, 6.45800964e-01, 1.74996967e+00, 1.28057606e+00,\n",
       "        2.01615911e-01, 1.41761187e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.15024861e-03,\n",
       "        1.22859696e-01, 9.35627350e-01, 1.89644966e+00, 9.46649706e-01,\n",
       "        1.16559198e-01, 3.15025452e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.57512726e-03, 1.08683577e-01,\n",
       "        9.54525328e-01, 1.82556907e+00, 8.80497792e-01, 1.00807955e-01,\n",
       "        3.15024861e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.45074582e-03, 2.58320386e-01,\n",
       "        1.26797506e+00, 1.70271577e+00, 6.04847732e-01, 5.19791020e-02]),\n",
       " array([-3.1730688e+00, -3.1095817e+00, -3.0460947e+00, -2.9826078e+00,\n",
       "        -2.9191208e+00, -2.8556337e+00, -2.7921467e+00, -2.7286596e+00,\n",
       "        -2.6651728e+00, -2.6016858e+00, -2.5381987e+00, -2.4747117e+00,\n",
       "        -2.4112246e+00, -2.3477378e+00, -2.2842507e+00, -2.2207637e+00,\n",
       "        -2.1572766e+00, -2.0937896e+00, -2.0303028e+00, -1.9668157e+00,\n",
       "        -1.9033287e+00, -1.8398416e+00, -1.7763547e+00, -1.7128676e+00,\n",
       "        -1.6493807e+00, -1.5858936e+00, -1.5224066e+00, -1.4589196e+00,\n",
       "        -1.3954326e+00, -1.3319457e+00, -1.2684586e+00, -1.2049716e+00,\n",
       "        -1.1414846e+00, -1.0779976e+00, -1.0145106e+00, -9.5102358e-01,\n",
       "        -8.8753659e-01, -8.2404959e-01, -7.6056254e-01, -6.9707555e-01,\n",
       "        -6.3358855e-01, -5.7010156e-01, -5.0661457e-01, -4.4312754e-01,\n",
       "        -3.7964052e-01, -3.1615353e-01, -2.5266653e-01, -1.8917951e-01,\n",
       "        -1.2569252e-01, -6.2205505e-02,  1.2814999e-03,  6.4768508e-02,\n",
       "         1.2825552e-01,  1.9174251e-01,  2.5522953e-01,  3.1871653e-01,\n",
       "         3.8220352e-01,  4.4569054e-01,  5.0917757e-01,  5.7266456e-01,\n",
       "         6.3615155e-01,  6.9963855e-01,  7.6312554e-01,  8.2661259e-01,\n",
       "         8.9009959e-01,  9.5358658e-01,  1.0170736e+00,  1.0805606e+00,\n",
       "         1.1440476e+00,  1.2075346e+00,  1.2710216e+00,  1.3345087e+00,\n",
       "         1.3979956e+00,  1.4614826e+00,  1.5249696e+00,  1.5884566e+00,\n",
       "         1.6519437e+00,  1.7154306e+00,  1.7789177e+00,  1.8424046e+00,\n",
       "         1.9058917e+00,  1.9693787e+00,  2.0328658e+00,  2.0963526e+00,\n",
       "         2.1598396e+00,  2.2233267e+00,  2.2868137e+00,  2.3503008e+00,\n",
       "         2.4137876e+00,  2.4772747e+00,  2.5407617e+00,  2.6042488e+00,\n",
       "         2.6677358e+00,  2.7312226e+00,  2.7947097e+00,  2.8581967e+00,\n",
       "         2.9216838e+00,  2.9851708e+00,  3.0486577e+00,  3.1121447e+00,\n",
       "         3.1756318e+00], dtype=float32),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(samples, density=True, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "be24d69c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.9713]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9722]]],\n",
      "\n",
      "\n",
      "        [[[-2.9564]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1035]]],\n",
      "\n",
      "\n",
      "        [[[-3.0913]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9144]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9837]]],\n",
      "\n",
      "\n",
      "        [[[-1.9785]]],\n",
      "\n",
      "\n",
      "        [[[-1.9306]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0437]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0342]]],\n",
      "\n",
      "\n",
      "        [[[-2.9603]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0607]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9920]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9464]]],\n",
      "\n",
      "\n",
      "        [[[-1.9937]]],\n",
      "\n",
      "\n",
      "        [[[-1.9368]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9184]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0329]]],\n",
      "\n",
      "\n",
      "        [[[-3.0150]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9757]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9786]]],\n",
      "\n",
      "\n",
      "        [[[-2.9751]]],\n",
      "\n",
      "\n",
      "        [[[-2.9558]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0355]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0862]]],\n",
      "\n",
      "\n",
      "        [[[-2.0256]]],\n",
      "\n",
      "\n",
      "        [[[-1.9912]]],\n",
      "\n",
      "\n",
      "        [[[-2.9279]]],\n",
      "\n",
      "\n",
      "        [[[-2.9879]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9682]]],\n",
      "\n",
      "\n",
      "        [[[-1.9050]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0319]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9592]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9607]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0041]]],\n",
      "\n",
      "\n",
      "        [[[-1.9990]]],\n",
      "\n",
      "\n",
      "        [[[-1.9707]]],\n",
      "\n",
      "\n",
      "        [[[-2.0606]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9531]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9868]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9433]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0841]]],\n",
      "\n",
      "\n",
      "        [[[-1.9700]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0053]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0071]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9574]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9029]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1036]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9691]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0387]]],\n",
      "\n",
      "\n",
      "        [[[-3.0176]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8775]]],\n",
      "\n",
      "\n",
      "        [[[-1.9483]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9421]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9530]]],\n",
      "\n",
      "\n",
      "        [[[-3.0465]]],\n",
      "\n",
      "\n",
      "        [[[-2.0465]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9461]]],\n",
      "\n",
      "\n",
      "        [[[-2.9366]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0162]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9426]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0524]]],\n",
      "\n",
      "\n",
      "        [[[-2.9459]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0008]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0794]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9422]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9564]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0131]]],\n",
      "\n",
      "\n",
      "        [[[-3.0316]]],\n",
      "\n",
      "\n",
      "        [[[-2.0367]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0554]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0245]]],\n",
      "\n",
      "\n",
      "        [[[-3.0177]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9326]]],\n",
      "\n",
      "\n",
      "        [[[-2.0145]]],\n",
      "\n",
      "\n",
      "        [[[-3.0934]]],\n",
      "\n",
      "\n",
      "        [[[-2.9793]]],\n",
      "\n",
      "\n",
      "        [[[-2.9845]]],\n",
      "\n",
      "\n",
      "        [[[-1.9944]]],\n",
      "\n",
      "\n",
      "        [[[-2.0096]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9646]]],\n",
      "\n",
      "\n",
      "        [[[-3.0290]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9169]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9631]]],\n",
      "\n",
      "\n",
      "        [[[-2.0766]]],\n",
      "\n",
      "\n",
      "        [[[-2.8900]]],\n",
      "\n",
      "\n",
      "        [[[-2.9772]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9743]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8676]]],\n",
      "\n",
      "\n",
      "        [[[-1.9590]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9930]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0334]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0573]]],\n",
      "\n",
      "\n",
      "        [[[-2.9686]]],\n",
      "\n",
      "\n",
      "        [[[-2.0337]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9092]]],\n",
      "\n",
      "\n",
      "        [[[-2.0148]]],\n",
      "\n",
      "\n",
      "        [[[-2.0454]]],\n",
      "\n",
      "\n",
      "        [[[-1.9666]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0261]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9993]]],\n",
      "\n",
      "\n",
      "        [[[-3.0613]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0798]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9805]]],\n",
      "\n",
      "\n",
      "        [[[-2.0019]]],\n",
      "\n",
      "\n",
      "        [[[-1.9739]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0822]]],\n",
      "\n",
      "\n",
      "        [[[-2.0631]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0216]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0835]]],\n",
      "\n",
      "\n",
      "        [[[-2.0465]]],\n",
      "\n",
      "\n",
      "        [[[-3.0472]]],\n",
      "\n",
      "\n",
      "        [[[-2.9792]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1012]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9630]]],\n",
      "\n",
      "\n",
      "        [[[-2.0199]]],\n",
      "\n",
      "\n",
      "        [[[-2.9868]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9776]]],\n",
      "\n",
      "\n",
      "        [[[-1.8545]]],\n",
      "\n",
      "\n",
      "        [[[-2.0232]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0200]]],\n",
      "\n",
      "\n",
      "        [[[-1.9677]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0458]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0232]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0273]]],\n",
      "\n",
      "\n",
      "        [[[-2.0721]]],\n",
      "\n",
      "\n",
      "        [[[-1.9204]]]])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "24aae55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.8607:   1%|█▌                                                                                                                                                                                                     | 79/10000 [00:00<01:24, 118.02it/s]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m step \u001b[38;5;241m<\u001b[39m train_num_steps:\n\u001b[1;32m     13\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 15\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdliter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m diffusion(data)\n\u001b[1;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ddgan/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/ddgan/lib/python3.9/site-packages/torch/utils/data/dataloader.py:720\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/miniconda3/envs/ddgan/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "train_num_steps = 10000\n",
    "device = torch.device(\"cuda:0\")\n",
    "diffusion.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(diffusion.parameters(), lr = 1e-4, betas = (0.9, 0.99))\n",
    "dliter = iter(dl)\n",
    "\n",
    "with tqdm(initial = step, total = train_num_steps) as pbar:\n",
    "\n",
    "    while step < train_num_steps:\n",
    "\n",
    "        total_loss = 0.\n",
    "\n",
    "        data = next(dliter).to(device)\n",
    "        loss = diffusion(data)\n",
    "        loss.backward()\n",
    "\n",
    "        pbar.set_description(f'loss: {loss.cpu().item():.4f}')\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        step += 1\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e7beb1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "height and width of image must be 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m training_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m) \u001b[38;5;66;03m# images are normalized from 0 to 1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# after a lot of training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddgan/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/denoising-diffusion/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py:679\u001b[0m, in \u001b[0;36mGaussianDiffusion.forward\u001b[0;34m(self, img, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    678\u001b[0m     b, c, h, w, device, img_size, \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m*\u001b[39mimg\u001b[38;5;241m.\u001b[39mshape, img\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m h \u001b[38;5;241m==\u001b[39m img_size \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;241m==\u001b[39m img_size, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight and width of image must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    680\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, (b,), device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    682\u001b[0m     img \u001b[38;5;241m=\u001b[39m normalize_to_neg_one_to_one(img)\n",
      "\u001b[0;31mAssertionError\u001b[0m: height and width of image must be 1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "training_images = torch.randn(8, 3, 128, 128) # images are normalized from 0 to 1\n",
    "loss = diffusion(training_images)\n",
    "loss.backward()\n",
    "# after a lot of training\n",
    "\n",
    "sampled_images = diffusion.sample(batch_size = 4)\n",
    "sampled_images.shape # (4, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f3063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
